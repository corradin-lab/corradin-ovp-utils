# AUTOGENERATED! DO NOT EDIT! File to edit: prefect_permutation_flow.ipynb (unless otherwise specified).

__all__ = ['read_parquet_single_pair_cache_df', 'load_file', 'get_pairs_for_step2', 'calculate_num_batches',
           'get_permute_odds_ratio_object', 'get_duplicated_pair_info', 'perform_permutation',
           'resolve_iteration_result_step_file_name', 'get_previous_steps_result', 'process_report_df',
           'determine_if_needs_more_perm', 'resolve_output_folder_path', 'parquet_result', 'tsv_result_partial',
           'mtc_file_name', 'iteration_result_step_file_template', 'file_path_template', 'flow_run_name_template',
           'template_summary_df_target_task', 'dynamic_executor', 'dask_executor']

# Cell
from prefect.engine.results import LocalResult
from prefect import task, Flow, Parameter, unmapped, flatten
from prefect.engine.serializers import PandasSerializer
from prefect.tasks.shell import ShellTask
from prefect.tasks.templates import StringFormatter
from prefect.tasks.prefect import create_flow_run, StartFlowRun, RenameFlowRun
from prefect.engine.executors import DaskExecutor, LocalDaskExecutor, LocalExecutor
from prefect.engine import signals
from prefect import context
context.config.flows.checkpointing = True

import numpy as np
import pandas as pd
from typing import Union, List, Literal
import math

# Cell
from .step1 import pairs_df_to_records, task_no_checkpoint
from ..permutations import PermuteOddsRatio
from ..odds_ratio import reconstruct_genetic_info
from ..MTC import MtcTable, StepwiseFilter
from .step1 import (generate_summary_df,
SINGLE_PAIR_DATA_CACHE_TEMPLATE_WITHOUT_EXTENSION,
template_summary_df_target,
STEP1_FINAL_REPORT_FILE_NAME, ALL_GENO_DF_FILE_NAME)
from fastcore.basics import partialler

# Cell
parquet_result = LocalResult("{step1_folder_path}", serializer = PandasSerializer("parquet"))
tsv_result_partial = LocalResult(serializer = PandasSerializer("csv",
                                                                         serialize_kwargs={"sep":"\t", "index": False},
                                                                         deserialize_kwargs={"sep": "\t"}))
mtc_file_name = "MTCFilterResult/1_MTC_table.tsv"
iteration_result_step_file_template = "{output_folder_path_resolved}/permutation_results/{GWAS_id}/{GWAS_id}_{total_iterations}_total_iterations"

@task_no_checkpoint(target = partialler(template_summary_df_target, parquet=True), result = parquet_result)
def read_parquet_single_pair_cache_df(pair_info):
#     single_pair_df_path = template_summary_df_target(pair_info=pair_info, parquet = True)
#     single_pair_df = parquet_result_partial().read(location=single_pair_df_path).value
    return single_pair_df

@task_no_checkpoint
def load_file(file_path, file_type="csv"):
    if file_type == "csv":
        result = LocalResult(serializer = PandasSerializer("csv", serialize_kwargs={"sep":"\t", "index": False}, deserialize_kwargs={"sep": "\t"})).read(location=file_path)
    if file_type == "parquet":
        result = LocalResult(serializer = PandasSerializer("parquet")).read(location=file_path)

    df = result.value
    return df

@task_no_checkpoint
def get_pairs_for_step2(step1_final_report_df, GWAS_ids : Union[str, List[str]] = None, outside_ids: Union[str, List[str]] = None ):
    step1_final_report_df_filtered = step1_final_report_df.query("geno_combo_passed_filter_2 == 1")
    if GWAS_ids:
        step1_final_report_df_filtered = step1_final_report_df_filtered.query("GWAS_id == @GWAS_ids")
    if outside_ids:
        step1_final_report_df_filtered = step1_final_report_df_filtered.query("GWAS_id == @outside_ids")
    pair_info_list = pairs_df_to_records.run(step1_final_report_df_filtered)
    return pair_info_list

@task_no_checkpoint
def calculate_num_batches(total_iterations:int, batch_size = 1_000):
    num_batches = math.ceil(total_iterations/batch_size)
    return num_batches

@task_no_checkpoint
def get_permute_odds_ratio_object(single_pair_df, pair_info, all_geno_df, num_batches):

    #if the sample ids are loaded as string it will cause error downstream
    assert type(single_pair_df.loc[0, "unique_samples_id_case"]) == np.ndarray
    return [PermuteOddsRatio(summary_df = single_pair_df, single_rsid=pair_info.GWAS_id, all_geno_df=all_geno_df, combo_rsid_list= [pair_info.GWAS_id, pair_info.outside_id], permute_rsid= pair_info.outside_id)\
            for batch in range(num_batches)]

@task_no_checkpoint
def get_duplicated_pair_info(pair_info, num_batches):
    return [pair_info for batch in range(num_batches)]


@task(target="{output_folder_path_resolved}/permutation_results/{pair_info.GWAS_id}/{pair_info.outside_id}/{pair_info.GWAS_id}_{pair_info.outside_id}_{total_iterations}_total_iterations_{n_iterations}_{map_index}",
      result = tsv_result_partial,
      checkpoint = True)
def perform_permutation(*, permute_odds_ratio_obj: PermuteOddsRatio, pair_info,
                        n_iterations: int,
                        total_iterations: int,
                        output_folder_path_resolved:str):
    iterations_performed = n_iterations if n_iterations < total_iterations else total_iterations
    permute_odds_ratio_obj.perform_permutation(n_iterations=iterations_performed)
    return permute_odds_ratio_obj.report_df


def resolve_iteration_result_step_file_name(*, output_folder_path_resolved, **kwargs):
    if kwargs["GWAS_id"] is None:
        iteration_result_step_file_name = "{output_folder_path_resolved}/permutation_results/all_GWAS_{total_iterations}_total_iterations_permutation_result".format(output_folder_path_resolved = output_folder_path_resolved, **kwargs)
    else:
        iteration_result_step_file_name = "{output_folder_path_resolved}/permutation_results/{GWAS_id}/{GWAS_id}_{total_iterations}_total_iterations".format(output_folder_path_resolved = output_folder_path_resolved, **kwargs)
    return iteration_result_step_file_name

@task_no_checkpoint
def get_previous_steps_result(iteration_steps: List[int], total_iterations: int, GWAS_id: str, output_folder_path_resolved):
    if total_iterations not in iteration_steps:
        raise signals.FAIL
    current_step_index = iteration_steps.index(total_iterations)
    previous_steps = iteration_steps[:current_step_index]
    previous_steps_result_files = [tsv_result_partial.read(location = resolve_iteration_result_step_file_name(output_folder_path_resolved = output_folder_path_resolved,
                                                                                                                   GWAS_id = GWAS_id,
                                                                                                                   total_iterations = iter_step)).value for iter_step in previous_steps]
    return previous_steps_result_files


# def template_permutation_df_target(pair_info):

# @task(target="{output_folder_path_resolved}/all_permutation_results", result = tsv_result_partial())
# def combine_results(report_dfs, output_folder_path_resolved):
#     all_results_df = pd.concat(report_dfs)
#     return all_results_df


@task(target=resolve_iteration_result_step_file_name, result = tsv_result_partial ,checkpoint = True)
def process_report_df(report_df_melted_list:List[pd.DataFrame],
                      mtc_df,
                      step1_final_report_df,
                      output_folder_path_resolved,
                      previous_steps_result_files: List[pd.DataFrame]):

    mtc_table_obj = MtcTable(mtc_df, "threshold")
    stepwise_filter_obj = StepwiseFilter(mtc_table_obj, 99)

    all_report_df_melted = pd.concat( previous_steps_result_files + report_df_melted_list)



    group_by_cols = ["GWAS_id", "GWAS_id_geno", "outside_id", "outside_id_geno", "combo_higher_than_single_str"]

    agg_df =  all_report_df_melted.groupby(group_by_cols)
    agg_df = agg_df.agg({"iterations": "sum",
                "iter_used_for_pval": "sum",
                 "num_perm_lower":"sum",
                 "num_perm_higher":"sum",
                }).reset_index()#.add_prefix("combined_").reset_index()

    agg_df["pval"] = agg_df["iter_used_for_pval"] / agg_df["iterations"]
    agg_df["pval"] = np.where(agg_df["pval"] != 0, agg_df["pval"].astype(float), 1/agg_df["iterations"].astype(float))

    #step1_final_report_df_filtered = step1_final_report_df.query("geno_combo_passed_filter_2 == 1")
    _, agg_df["ci_neg"],  agg_df["c_i_pos"], agg_df["mtc_threshold"], agg_df["need_more_perm"], agg_df["status"] = stepwise_filter_obj.get_compare_info_list_vectorized(agg_df["iterations"],
    agg_df["pval"],
    agg_df["GWAS_id"],
    agg_df["combo_higher_than_single_str"])



    #all_report_df_melted = all_report_df_melted.drop(columns = ["iterations", "iter_used_for_pval", "num_perm_lower", "num_perm_higher", "pval"]).drop_duplicates()
    # all_report_df_melted = all_report_df_melted[["GWAS_id", "GWAS_id_geno", "outside_id", "outside_id_geno", 'GWAS_chrom', 'all_SNPs_found_in_genetic_file', 'case_total_no_NA',
    #    'case_total_with_NA', 'combo_higher_than_single_binary',  "combo_higher_than_single_str",
    #    'control_total_no_NA', 'control_total_with_NA',
    #    'found_in_genetic_file_GWAS', 'found_in_genetic_file_outside',
    #    'geno_combo_passed_filter_1', 'geno_combo_passed_filter_2',
    # 'odds_ratio_combo', 'odds_ratio_single',
    #    'outside_chrom', 'pair_has_enough_geno_combo_passed_filter_1',
    #    'unique_samples_count_case', 'unique_samples_count_control']].dropna().drop_duplicates()

    #breakpoint()
    all_report_df_melted = all_report_df_melted[['GWAS_id', 'GWAS_id_geno', 'outside_id', 'outside_id_geno',
       'combo_higher_than_single_binary', 'combo_higher_than_single_str',
       'odds_ratio_combo', 'odds_ratio_single']].dropna().drop_duplicates()
    #breakpoint()
    all_report_df_melted = all_report_df_melted.merge(agg_df)



    #final_report_df = report_df_melted.merge(step1_final_report_df_filtered, how = "left")
    final_report_df = step1_final_report_df.merge(all_report_df_melted, how = "right")
    return final_report_df

@task_no_checkpoint
def determine_if_needs_more_perm(final_report_df):
    need_more_perm = (final_report_df.query("geno_combo_passed_filter_2 == 1")["status"] == "more_perm").any()
    return need_more_perm, final_report_df.query("geno_combo_passed_filter_2 == 1 and status == 'more_perm'")

@task_no_checkpoint
def resolve_output_folder_path(step1_folder_path, output_folder_path):
    if output_folder_path is None:
        output_folder_path_resolved = step1_folder_path
    else:
        output_folder_path_resolved = output_folder_path
    return output_folder_path_resolved

file_path_template = StringFormatter(name = "create_file_path", template = "{folder_path}/{file_name}")
flow_run_name_template= StringFormatter(name = "create_flow_name", template = "{GWAS_id}_total_iter_{total_iterations}")
template_summary_df_target_task = task()(template_summary_df_target)



# Cell
def dynamic_executor():#executor_type: Literal["local_dask", "cluster_dask", "default"] = "default"):
    executor_type = context.parameters["executor"]
    if executor_type == "local_dask":
        executor = LocalDaskExecutor()
    elif executor_type == "cluster_dask":
        executor = DaskExecutor('tcp://172.18.102.9:8789')
    elif executor_type == "default":
        executor = None
    return executor





# Cell
from prefect.executors import DaskExecutor
dask_executor = DaskExecutor(address="tcp://172.18.102.9:8789")

# Cell

#maximum iter
#
with Flow("OVP_step2", result = LocalResult(dir = "./prefect_can_delete"), executor =dask_executor ) as dev_flow:
#with Flow("OVP_step2", result = LocalResult(dir = "./prefect_can_delete")) as dev_flow:
    step1_folder_path = Parameter("step1_folder_path")
    output_folder_path = Parameter("output_folder_path", default = None)
    GWAS_id = Parameter("GWAS_id", default = None)
    outside_id = Parameter("outside_id", default = None)
    n_iterations = Parameter("n_iterations")
    total_iterations = Parameter("total_iterations")
    iteration_steps = Parameter("iteration_steps", default = [1_000, 10_000, 100_000, 1_000_000])
    dev_flow.add_task(Parameter("executor", default = "default"))

#     flow_run_name = flow_run_name_template()
#     new_name = RenameFlowRun(flow_run_id=context.get("flow_run_id"),flow_run_name= flow_run_name)()
    output_folder_path_resolved = resolve_output_folder_path(step1_folder_path, output_folder_path=output_folder_path)

    step1_final_report_file_path = file_path_template(folder_path = step1_folder_path, file_name = STEP1_FINAL_REPORT_FILE_NAME)
    all_geno_df_file_path = file_path_template(folder_path = step1_folder_path, file_name = ALL_GENO_DF_FILE_NAME)
    mtc_df_file_path = file_path_template(folder_path = step1_folder_path, file_name = mtc_file_name)

    step1_final_report_df = load_file(step1_final_report_file_path)
    all_geno_df = load_file(all_geno_df_file_path, file_type = "parquet")
    mtc_df = load_file(mtc_df_file_path)

    pair_info_list = get_pairs_for_step2(step1_final_report_df, GWAS_ids = GWAS_id, outside_ids= outside_id)
    single_pair_df_paths = template_summary_df_target_task.map(pair_info = pair_info_list, parquet = unmapped(True))
    single_pair_dfs = load_file.map(file_path = file_path_template.map(folder_path = unmapped(step1_folder_path), file_name = single_pair_df_paths), file_type = unmapped("parquet"))

    num_batches = calculate_num_batches(total_iterations= total_iterations, batch_size= n_iterations)
    permute_odds_ratio_objects = get_permute_odds_ratio_object.map(single_pair_df=single_pair_dfs,
                                                                pair_info = pair_info_list,
                                                                all_geno_df= unmapped(all_geno_df),
                                                                num_batches = unmapped(num_batches))
    duplicated_pair_info_list = get_duplicated_pair_info.map(pair_info = pair_info_list,
                                                             num_batches = unmapped(num_batches))

    report_dfs = perform_permutation.map(permute_odds_ratio_obj= flatten(permute_odds_ratio_objects),
                                         n_iterations=unmapped(n_iterations),
                                         total_iterations=unmapped(total_iterations),
                                         pair_info = flatten(duplicated_pair_info_list),
                                         output_folder_path_resolved=unmapped(output_folder_path_resolved))

    report_dfs_melted = generate_summary_df.map(single_pair_cache_df = report_dfs, pair_info = flatten(duplicated_pair_info_list))

    # final_report_dfs = process_report_df.map(report_df = report_dfs_melted, #pair_info = flatten(duplicated_pair_info_list),
    #                                          mtc_df=unmapped(mtc_df),
    #                                          step1_final_report_df= unmapped(step1_final_report_df))

    previous_steps_result_files = get_previous_steps_result(iteration_steps= iteration_steps, total_iterations = total_iterations, GWAS_id = GWAS_id, output_folder_path_resolved = output_folder_path_resolved)
    final_report_dfs = process_report_df(report_df_melted_list= report_dfs_melted, mtc_df = mtc_df, step1_final_report_df = step1_final_report_df, output_folder_path_resolved = output_folder_path_resolved, previous_steps_result_files = previous_steps_result_files)

    #read_parquet_single_pair_cache_df()

#flow.add_task(ShellTask("pwd"))
dev_flow.visualize()